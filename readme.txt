
# Chess_NN

Chess_NN is chess engine powered by an ensemble of neural networks.  It has a web-app 
frontend, playable in a browser, by virtue of Python's Django web framework.  When 
given a board state, it attempts to predict what the best move would be, as according 
to the Stockfish chess engine, and then plays that move.

Stockfish is the dominant chess engine, capable of beating the best human player
98% of the time.  It does this by searching through the exponentially-expanding tree
of possible moves, for as many future turns as it can.  Typically users try to reduce
its abilities by limiting the time allowed for this search, or making it occasionally
play non-optimum moves.

When tested on previously unseen board-states, Chess_NN currently predicts Stockfish's 
optimum move for 40% of boards, and does so roughly 1000x faster.  However, it is also
unable to predict any move 30% of the time, so resorts to making a random legal move.
In the remaining 30% of cases, it plays a legal move of indeterminate quality.

## How A Neural Network Learns Chess

"Everything in chess is pattern recognition" - Intl.Master Robert Ris

The training data for the neural network is taken from Lichess.com's puzzles database,
which contains 1.2 million mid-game and end-game board states taken from human games,
with the 'correct answer' being assessed by Stockfish.  The training data does not 
contain any openings (the first 10 or so moves by each player), hence the user is
prompted to choose from a selection of fully-developed openings that give both players 
a fair chance of winning.

The neural network is based on the Keras Functional API, which allows the user to 
buildmore complex networks with non-standard topologies.  The ChessNN input layer 
accepts a 64 x 13 one-hot array (64 squares than can be filled with one of 6 black, 
6 white, or no pieces) and the output is multiple (64) multi-class classifiers (the 13 
piece options are presented as a probability vector to a soft-max function).  

Testing showed that wide and shallow networks performed better (and trained faster) 
than deeper networks with a similar number of neurons, and that injecting a low level 
of noise into the input layer improved it ability to generalise, at the expense of 
training time.

Experiments with adding noise during solving, then taking the average of all solutions
led to training several models on a portion of the training data each.  By combining
the resulting predictions in various ways we can avoid many instances of the two most
common failure modes: a piece being cloned during a move, or a piece disappearing
during a move.

## But... Is Chess_NN actually any good at playing chess?

Not really.  30% of its moves 
involve a random pick from a list of legal moves (generated by the python-chess 
library).  This happens when all the stages of the ensemble's voting criteria have 
been exhausted, without producing a sensible result.  In real gameplay, this means
every so often the model will play several random moves in a row, which can be taken 
advantage of by an average human player, leading to an unsurmountable upper hand.

However... It is possible to extract six or more times as much training data from 
the Lichess.com puzzle sequences (I only used the first step of each one).  And, if 
more RAM was made available much larger networks could be trained. Making both these 
improvements would almost certainly result in a significant boost in performance.

## What's Under The Hood

_0_chess_tools.py  Contains functions shared by the other python files, when 
converting between various data formats, checking for illegal moves, displaying the
game state, or comparing move predictions to Stockfish ground truths.

_1_parser.py  Reads chess puzzles written in Forsyth-Edwards notation from 
Lichess.com's puzzle database.  Data is cleaned and parsed according to puzzle-type.  
Black-to-play games converted into white-to-play for consistency during training.

_2_encoder.py  Takes the parsed FENs and converts them into one-hot tensors.  The 
one-hot tensors are an array with 64 rows (each represents a square on the chess 
board) and 13 columns (each represents a possible piece that can occupy a square).  
These are a sparse data format, containing mostly zeroes. 
Applying the first move from the Lichess data creates the x_data, then applying 
the second move creates the y_data

_3_data_viewer.py  When given a one-hot tensor, creates .png image of the chess 
board.  Used to check the training data is as expected / free of errors.

_4_trainer.py  Creates training, validation, and testing datasets containing one-
hot tensors.  Initialises a neural network, trains it, assesses its ability to find
the best move, then saves the model.  Plots the training history in order to help
diagnose over-training or an under-powered network.

_5_solver.py  Picks random puzzles from the testing dataset, and compares the 
neural network's prediction of the solution against the ground truth solution from 
the database.  Four puzzle-solution-prediction triplets are converted to a graphic 
and saved as a .png file.  These can be found in the /results/ folder.

_6_solver_ensemble.py  Several neural networks are given the same chess board.  The 
predicted solutions are combined according to the decision criteria to produce an 
output that is substantially more accurate than any single neural network is able 
to produce.

_7_ensemble_graph_maker.py  Visualises the improvements made when increasing the
number of neural networks in the ensemble.  An example can be found at /training 
graphs/general/GS ensemble graph.png

m2_.. .py  Does the same things as the above files, but only considers a subset of
the Lichess database: 'Mate In Two' puzzles.